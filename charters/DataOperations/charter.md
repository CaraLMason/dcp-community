# [Data Operations](mailto:data-operations-team@data.humancellatlas.org)


## Description

The Data Coordination Platform (DCP) Data Operations Team is responsible for providing the scientific community with effective tools and resources which maximize the value and use of HCA data. This involves identifying the value added for the HCA consumers from various DCP components and the needs and expectations of the consumers, and connecting these communities through useful, fully vetted products and their distribution.

## Definitions [Optional]

Data submission: a set of raw data derived from the same experimental process, and the metadata describing that process
Data release: the point at which metadata and data from a single data submission become publicly available
Data distribution: the packaging of metadata and data from multiple data submissions in meaningful ways and any announcements or documentation supporting it

## Objectives

The objective of the Data Operations is to provide the scientific community with effective tools and resources from the DCP which maximize the value and use of HCA data

## In-scope

- Data operations
  - Engage with all DCP components to identify deliverables, including data, pipelines, metadata, standards, and user features, which maximize the value and consistency of HCA data
  - Coordinate DCP-wide end-to-end testing pre-release and continued QC checks post-release to ensure data and data resource integrity and useability through component updates
  - Identify data release- and data distribution-associated engineering needs of all DCP components and participate in crafting technical specifications
  - Establish and integrate into the DCP roadmap a reproducible plan that includes the specifications for and the timing of iterative data distribution to the community
  - Monitor the status of data submissions at each step in the DCP data workflow from initial identification of suitability through data release and potential inclusion in additional data distribution
  - Engage with Governance Group and Analysis Work Group to identify data quality standards which dictate data eligible to be released and data eligible to be included in data distributions
  - Engage with compliance to ensure that the data release and data distribution processes are facilitating appropriate data access to all of the types of consumers
  - Ensure released data are useable to tertiary portals and users can easily cross-reference across portals
  - Determine best practices for when and how changes are made to DCP outputs after release

- Community engagement
  - Define data use policy and citation instructions for HCA data users
  - Define mechanism(s) for which the community is notified when DCP products are updated after release
  - Partner with Content and Portal Development to ensure clear, findable, and current documentation describing data release and data distribution
  - Ensure effective communication of HCA data updates and user features to the scientific community
  - Partner with UX to assess curation added value of data distribution through communitiesâ€™ feedback, and better understand their expectations and needs
  - Onboard staff to DCP ZenDesk ticketing system
  - Take ownership of establishing process and interfacing with DCP components for assigning, responding, and integrating ZenDesk tickets
  - Compile feedback gained from consumers through ZenDesk queries and comments, and report back to the DCP in order to facilitate system improvement 



### Scientific "guardrails" [Optional]

In order to meet the needs of the communities of scientific consumers, the DataOps team requires input from the Governance Group as to where curated data distribution will add value for the researcher.

## Out-of-scope

The Data Operations is not responsible for determining what data or data types are made available through the HCA.

As many of the features and requirements for data release and data distribution will be implemented through other components, the majority of technical aspects pertaining to data release and data distribution will be implemented by the appropriate component, and not Data Operations.

## Milestones and Deliverables

2019 Q3
- Follow a dataset through the entire data workflow in order to understand each step and assess the current process. 
- Organize incentivized data submission program in conjunction with the HCA General Meeting in Barcelona in order to encourage pre-publication data submission. 
- Establish ZenDesk ticket triage procedure.

2019 Q4
- Write actional RFCs aimed to improve the data workflow.

2020 Q1
- Determine a reproducible procedure for periodic data distribution including:
  - How to determine what data to include
  - How the data are organized
  - How to document and announce the distribution.


## Roles

### Project Lead

[Jason Hilton](mailto:jahilton@stanford.edu)

### Product Owner

[Jennifer Zamanian](mailto:jlz@stanford.edu)

### Technical Lead

[J. Seth Strattan](mailto:jseth@stanford.edu)

## Communication
### Slack Channels
[HumanCellAtlas/data-ops](https://humancellatlas.slack.com/messages/data-ops)

### Mailing List
Team email: data-operations-team@data.humancellatlas.org
